#!/usr/bin/env python3
"""
main.py â€” Ù†Ù‚Ø·Ø© ØªØ´ØºÙŠÙ„ Ù†Ø¸Ø§Ù… OffloadHelper ÙÙŠ Ù…Ù„Ù ÙˆØ§Ø­Ø¯
Ø®ÙŠØ§Ø±Ø§Øª Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±:
  -s / --stats-interval  Ø«ÙˆØ§Ù†ÙŠ Ø¨ÙŠÙ† ÙƒÙ„ Ø·Ø¨Ø§Ø¹Ø© Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ø§Ù„Ø£Ù‚Ø±Ø§Ù† (0 = Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)
  --no-cli               ØªØ´ØºÙŠÙ„ Ø¨Ù„Ø§ Ù‚Ø§Ø¦Ù…Ø© ØªÙØ§Ø¹Ù„ÙŠØ© Ø­ØªÙ‰ Ù…Ø¹ ÙˆØ¬ÙˆØ¯ TTY
"""
import os
import sys
import time
import threading
import subprocess
import logging
import argparse
from pathlib import Path
from typing import Any

from flask import Flask, request, jsonify
from flask_cors import CORS

# ØªØ´ØºÙŠÙ„ external_server.py ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§
def start_external_server():
    try:
        logging.info("ğŸš€ ØªØ´ØºÙŠÙ„ external_server.py ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§...")
        subprocess.Popen([sys.executable, os.path.join(os.getcwd(), "external_server.py")])
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ external_server.py: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ø¶Ø¨Ø· Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FILE = Path(__file__).resolve()
BASE_DIR = FILE.parent
PROJECT_ROOT = BASE_DIR.parent
for p in (BASE_DIR, PROJECT_ROOT):
    sys.path.insert(0, str(p))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/main.log", mode="a")
    ]
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from dotenv import load_dotenv
    load_dotenv()
    logging.info("ğŸ”§ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ù…Ù† â€.env")
except ImportError:
    logging.warning("ğŸ”§ python-dotenv ØºÙŠØ± Ù…Ø«Ø¨Ù‘ÙØªØ› ØªÙØ®Ø·Ù‘ÙŠ .env")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from peer_discovery import (
        register_service_lan,
        discover_lan_loop,
        register_with_central,
        fetch_central_loop,
        PEERS
    )
    from your_tasks import matrix_multiply, prime_calculation, data_processing
    from distributed_executor import DistributedExecutor
    from auto_offload import AutoOffloadExecutor
    from peer_statistics import print_peer_statistics
    from processor_manager import ResourceMonitor
except ImportError as e:
    logging.error(f"âŒ ØªØ¹Ø°Ù‘Ø± Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ­Ø¯Ø©: {e}")
    sys.exit(1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ø«Ø§Ø¨ØªØ§Øª Ø§Ù„ØªÙ‡ÙŠØ¦Ø© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if os.getenv("RENDER", "false") != "true":
    subprocess.Popen([PYTHON_EXE, "peer_server.py", "--port", str(CPU_PORT)])
CPU_PORT = int(os.getenv("PORT", "7520"))

SHARED_SECRET = os.getenv("SHARED_SECRET", "my_shared_secret_123")
PYTHON_EXE = sys.executable

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ø®ÙŠØ§Ø±Ø§Øª Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser()
parser.add_argument(
    "--stats-interval", "-s",
    type=int,
    default=0,
    help="Ø«ÙˆØ§Ù†ÙŠ Ø¨ÙŠÙ† ÙƒÙ„ Ø·Ø¨Ø§Ø¹Ø© Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ø§Ù„Ø£Ù‚Ø±Ø§Ù† (0 = Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)"
)
parser.add_argument(
    "--no-cli",
    action="store_true",
    help="ØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© Ø­ØªÙ‰ Ø¹Ù†Ø¯ ÙˆØ¬ÙˆØ¯ TTY"
)
args = parser.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ø®Ø§Ø¯Ù… Flask â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
flask_app = Flask(__name__)
CORS(flask_app, resources={r"/*": {"origins": "*"}})

@flask_app.route("/run_task", methods=["POST"])
def run_task():
    try:
        data = request.get_json() if request.is_json else request.form
        task_id = data.get("task_id")
        
        if not task_id:
            return jsonify(error="ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ task_id"), 400

        if task_id == "1":
            result = matrix_multiply(500)
        elif task_id == "2":
            result = prime_calculation(100_000)
        elif task_id == "3":
            result = data_processing(10_000)
        else:
            return jsonify(error="Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù‡Ù…Ø© ØºÙŠØ± ØµØ­ÙŠØ­"), 400

        return jsonify(result=result)

    except Exception as e:
        logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‡Ù…Ø©: {str(e)}", exc_info=True)
        return jsonify(error="Ø­Ø¯Ø« Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ ÙÙŠ Ø§Ù„Ø®Ø§Ø¯Ù…"), 500

def start_flask_server():
    ip_public = os.getenv("PUBLIC_IP", "127.0.0.1")
    logging.info(f"ğŸŒ Flask Ù…ØªÙˆÙØ± Ø¹Ù„Ù‰: http://{ip_public}:{CPU_PORT}/run_task")
    flask_app.run(host="0.0.0.0", port=CPU_PORT, debug=False)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ø®Ø¯Ù…Ø§Øª Ø®Ù„ÙÙŠØ© Ù…Ø­Ù„ÙŠØ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def start_services():
    try:
        subprocess.Popen([PYTHON_EXE, "peer_server.py", "--port", str(CPU_PORT)])
        subprocess.Popen([PYTHON_EXE, "load_balancer.py"])
        logging.info("âœ… ØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø®Ù„ÙÙŠÙ‘Ø©")
    except Exception as exc:
        logging.error(f"âŒ Ø®Ø·Ø£ Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø®Ù„ÙÙŠØ©: {exc}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ù…Ù‡Ø§Ù… Ù…Ø«Ø§Ù„ÙŠØ© Ù…Ø­Ù„ÙŠØ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def example_task(x: int) -> int:
    return x * x

def benchmark(fn, *args):
    t0 = time.time()
    res = fn(*args)
    return time.time() - t0, res

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø­Ù…Ù„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def auto_monitor(auto_executor):
    while True:
        try:
            monitor = ResourceMonitor().current_load()
            avg_cpu = monitor["average"]["cpu"]
            avg_mem = monitor["average"]["mem_percent"] if "mem_percent" in monitor["average"] else 0

            if avg_cpu > 0.7 or avg_mem > 85:
                logging.info("âš ï¸ Ø§Ù„Ø­Ù…Ù„ Ù…Ø±ØªÙØ¹ - Ø£ÙˆÙÙ„ÙˆØ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠ")
                auto_executor.submit_auto(example_task, 42, task_type="video")
            elif avg_cpu < 0.3:
                logging.info("âœ… Ø§Ù„Ø­Ù…Ù„ Ù…Ù†Ø®ÙØ¶ - Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ù…Ù‡Ø§Ù…")
            time.sleep(5)
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©: {e}")
            time.sleep(5)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def menu(executor: DistributedExecutor):
    tasks = {
        "1": ("Ø¶Ø±Ø¨ Ø§Ù„Ù…ØµÙÙˆÙØ§Øª", matrix_multiply, 500),
        "2": ("Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©", prime_calculation, 100_000),
        "3": ("Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", data_processing, 10_000),
        "5": ("Ù…Ù‡Ù…Ø© Ù…ÙˆØ²Ø¹Ø© (Ù…Ø«Ø§Ù„)", example_task, 42),
    }

    while True:
        print("\nğŸš€ Ù†Ø¸Ø§Ù… ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ")
        for k, (title, _, _) in tasks.items():
            print(f"{k}: {title}")
        print("q: Ø®Ø±ÙˆØ¬")
        choice = input("Ø§Ø®ØªØ± Ø§Ù„Ù…Ù‡Ù…Ø©: ").strip().lower()

        if choice == "q":
            print("ğŸ›‘ ØªÙ… Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬.")
            break
        if choice not in tasks:
            print("âš ï¸ Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ­ÙŠØ­.")
            continue

        name, fn, arg = tasks[choice]
        print(f"\nØªØ´ØºÙŠÙ„: {name}â€¦")

        try:
            if choice == "5":
                logging.info("ğŸ“¡ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‡Ù…Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ù…ÙˆØ²Ù‘ÙØ¹Ø©â€¦")
                future = executor.submit(fn, arg)
                print(f"âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø© (Ù…ÙˆØ²Ø¹Ø©): {future.result()}")
            else:
                dur, res = benchmark(fn, arg)
                print(f"âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©: {res}\nâ±ï¸ Ø§Ù„ÙˆÙ‚Øª: {dur:.3f}Â Ø«")
        except Exception as exc:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø©: {exc}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    # ØªØ´ØºÙŠÙ„ external_server Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù…
    start_external_server()

    start_services()

    executor = DistributedExecutor(SHARED_SECRET)
    auto_executor = AutoOffloadExecutor(executor)
    executor.peer_registry.register_service("node_main", CPU_PORT)

    for peer_url in list(PEERS):
        try:
            host, port_str = peer_url.split("//")[1].split("/run")[0].split(":")
            executor.peer_registry.register_service(
                f"peer_{host.replace('.', '_')}",
                int(port_str)
            )
        except Exception as exc:
            logging.warning(f"âš ï¸ ØªØ®Ø·Ù‘ÙŠ peer ({peer_url}): {exc}")

    initial_peers = [
        {"ip": host, "port": int(port)}
        for peer_url in PEERS
        if (hp := peer_url.split("//")[1].split("/run")[0]).count(":") == 1
        for host, port in [hp.split(":")]
    ]
    print_peer_statistics(initial_peers)

    if args.stats_interval > 0:
        threading.Thread(
            target=stats_loop,
            args=(args.stats_interval, executor),
            daemon=True
        ).start()

    logging.info("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„")

    threading.Thread(target=auto_monitor, args=(auto_executor,), daemon=True).start()

    if not args.no_cli and sys.stdin.isatty():
        menu(executor)
    else:
        logging.info("â„¹ï¸ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© Ù…Ø¹Ø·Ù‘Ù„Ø© (no TTY Ø£Ùˆ --no-cli)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    threading.Thread(target=register_service_lan, daemon=True).start()
    threading.Thread(target=discover_lan_loop, daemon=True).start()

    register_with_central()
    threading.Thread(target=fetch_central_loop, daemon=True).start()

    try:
        from internet_scanner import internet_scanner
        threading.Thread(
            target=internet_scanner.start_continuous_scan,
            daemon=True
        ).start()
        logging.info("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø³Ø­ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù„Ø¥Ù†ØªØ±Ù†Øª")
    except ImportError:
        logging.warning("ğŸ” internet_scanner ØºÙŠØ± Ù…ØªÙˆØ§ÙØ± â€“ ØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ")

    threading.Thread(target=start_flask_server, daemon=True).start()

    try:
        from your_control import control
        control.start()
    except ImportError:
        logging.info("ğŸ›ˆ your_control ØºÙŠØ± Ù…ØªÙˆÙÙ‘Ø± â€“ ØªØ´ØºÙŠÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠ")

    main()
  import os
import json
import torch
import subprocess
from transformers import AutoTokenizer, AutoModelForCausalLM
from responses import generate_reply

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ TinyLlama
tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
model = AutoModelForCausalLM.from_pretrained(
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    torch_dtype=torch.float16,
    device_map="auto"
)

# ØªØ­Ù…ÙŠÙ„ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
history_path = "history.json"

if os.path.exists(history_path):
    with open(history_path, "r", encoding="utf-8") as f:
        chat_history = json.load(f)
else:
    chat_history = []

# ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
def format_chat(history):
    messages = [
        {"role": "system", "content": "Ø£Ù†Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù†ÙˆØ±Ø§. ØªØ­Ø¯Ø«ÙŠ Ø¨Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙØµØ­Ù‰ Ø¨Ø³ÙŠØ·Ø©."}
    ]
    messages.extend(history)
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TinyLlama
def generate_llama_response(prompt, max_new_tokens=500):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        do_sample=True
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø®ÙˆØ§Ø¯Ù… (Ù…Ø­Ø§ÙƒØ§Ø©)
def simulate_server_scan():
    print("Ù†ÙˆØ±Ø§: Ø£Ø¨Ø­Ø« Ø¹Ù† Ø®ÙˆØ§Ø¯Ù…...")
    fake_servers = ["192.168.1.5", "192.168.1.10", "192.168.1.20"]
    for server in fake_servers:
        print(f"Ù†ÙˆØ±Ø§: ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø®Ø§Ø¯Ù… Ù…ÙØªÙˆØ­ ÙÙŠ {server}")

# Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
def chat():
    global chat_history

    print("""
    Ù†Ø¸Ø§Ù… Ù†ÙˆØ±Ø§ Ø§Ù„Ø°ÙƒÙŠ (Ø§Ù„Ø¥ØµØ¯Ø§Ø± TinyLlama)
    Ø£ÙˆØ§Ù…Ø± Ø®Ø§ØµØ©:
    - scan: Ù…Ø³Ø­ Ø§Ù„Ø´Ø¨ÙƒØ© (Ù…Ø­Ø§ÙƒØ§Ø©)
    - Ø®Ø±ÙˆØ¬/exit/quit: Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    """)

    while True:
        try:
            user_input = input("Ø£Ù†Øª: ").strip()
            if not user_input:
                continue

            if user_input.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
                break
                
            if user_input.lower() == "scan":
                simulate_server_scan()
                continue

            # Ø£ÙˆÙ„Ø§Ù‹: Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ù…Ù† responses.py
            custom_reply = generate_reply(user_input, username="Ø£Ø³Ø§Ù…Ø©")
            if custom_reply:
                print("Ù†ÙˆØ±Ø§:", custom_reply)
                chat_history.append({"role": "user", "content": user_input})
                chat_history.append({"role": "assistant", "content": custom_reply})
                continue

            # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø±Ø¯ Ø°ÙƒÙŠØŒ Ø§Ø³ØªØ®Ø¯Ù… TinyLlama
            chat_history.append({"role": "user", "content": user_input})
            prompt = format_chat(chat_history)
            
            print("Ù†ÙˆØ±Ø§: Ø£ÙÙƒØ±...")
            response = generate_llama_response(prompt)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ø±Ø¯ (Ù„Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¹ÙŠØ¯ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙƒØ§Ù…Ù„Ø§Ù‹)
            assistant_response = response.split("assistant\n")[-1].strip()
            print("Ù†ÙˆØ±Ø§:", assistant_response)
            
            chat_history.append({"role": "assistant", "content": assistant_response})

            # Ø­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„ ÙƒÙ„ 3 Ø±Ø³Ø§Ø¦Ù„ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©
            if len(chat_history) % 3 == 0:
                with open(history_path, "w", encoding="utf-8") as f:
                    json.dump(chat_history, f, ensure_ascii=False, indent=2)

        except KeyboardInterrupt:
            print("\nÙ†ÙˆØ±Ø§: ØªÙ… Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.")
            break
        except Exception as e:
            print(f"Ù†ÙˆØ±Ø§: Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
            continue

    # Ø­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§Ù„Ø®Ø±ÙˆØ¬
    with open(history_path, "w", encoding="utf-8") as f:
        json.dump(chat_history, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    chat()

